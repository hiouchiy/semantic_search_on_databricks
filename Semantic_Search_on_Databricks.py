# Databricks notebook source
# MAGIC %md 
# MAGIC ## 環境
# MAGIC - Runtime: 14.2 ML
# MAGIC - Node type: i3.2xlarge (Single Node)

# COMMAND ----------

# MAGIC %md
# MAGIC ## 0. 事前準備

# COMMAND ----------

# DBTITLE 1,必要な外部ライブラリのインストール 
# MAGIC %pip install mlflow==2.9.0 lxml==4.9.3 transformers==4.30.2 langchain==0.0.344 databricks-vectorsearch==0.22 databricks-sdk==0.12.0 databricks-sql-connector
# MAGIC dbutils.library.restartPython()

# COMMAND ----------

# DBTITLE 1,コンフィグ(環境に合わせて修正してください）
catalog = "YOUR_CATALOG_NAME"
schema = "YOUR_SCHEMA_NAME"
raw_data_table_name = "raw_query_data_table"
embed_table_name = "airbricks_documentation"

VECTOR_SEARCH_ENDPOINT_NAME="YOUR_VS_ENDPOINT_NAME"

embedding_endpoint_name = "databricks-bge-large-en"

# COMMAND ----------

# DBTITLE 1,カタログ初期化及びデモ用のインポートとヘルパーのインストール
# MAGIC %run ./_resources/00-init $reset_all_data=false

# COMMAND ----------

# MAGIC %md
# MAGIC ## 1. データのダウンロードとテーブル化

# COMMAND ----------

# MAGIC %md
# MAGIC ### 1.1. 元データ格納用のテーブル作成

# COMMAND ----------

# DBTITLE 1,Python変数をSQLで使用するための設定
spark.conf.set("my.catalogName", catalog)
spark.conf.set("my.schemaName", schema)
spark.conf.set("my.embbedTableName", embed_table_name)

# COMMAND ----------

# DBTITLE 1,データ格納用のテーブル定義
# MAGIC %sql
# MAGIC
# MAGIC -- カタログとスキーマが存在しなければ作成
# MAGIC CREATE CATALOG IF NOT EXISTS ${my.catalogName};
# MAGIC USE CATALOG ${my.catalogName};
# MAGIC CREATE SCHEMA IF NOT EXISTS ${my.catalogName}.${my.schemaName};
# MAGIC USE SCHEMA ${my.schemaName};
# MAGIC
# MAGIC -- テーブルが存在していれば一度削除
# MAGIC DROP TABLE IF EXISTS ${my.embbedTableName};
# MAGIC
# MAGIC -- テーブル定義。インデックスを作成するには、テーブルのChange Data Feedを有効にする必要がある。
# MAGIC CREATE TABLE IF NOT EXISTS ${my.embbedTableName} (
# MAGIC   id BIGINT GENERATED BY DEFAULT AS IDENTITY,
# MAGIC   query STRING,
# MAGIC   response STRING
# MAGIC ) TBLPROPERTIES (delta.enableChangeDataFeed = true); 

# COMMAND ----------

# MAGIC %md
# MAGIC ### 1.2. 元データのダウンロードおよびテーブルへのロード

# COMMAND ----------

raw_data_url = "https://raw.githubusercontent.com/hiouchiy/Pratical_RAG_Project/main/airbricks/query.json"
!wget $raw_data_url -O /tmp/query.json

spark.read.option("multiline","true").json('file:/tmp/query.json').write.mode('overwrite').saveAsTable(embed_table_name)

display(spark.table(embed_table_name))

# COMMAND ----------

# MAGIC %md
# MAGIC ## 2. ベクトルDBの作成

# COMMAND ----------

# MAGIC %md
# MAGIC （注）このデモでは埋め込みモデルとしてデータブリックスがホストしている基盤モデルの中から`databricks-bge-large-en`を使用します。
# MAGIC しかし、こちらは日本語に最適化されているモデルではないため、日本語を取り扱う際の精度を追求する場合は外部サービスやOSSの埋め込みモデルの使用を検討ください。
# MAGIC なお、OSS埋め込みモデルをデータブリックス上でサービングする手順は以下のURLをご参照ください。
# MAGIC
# MAGIC https://github.com/hiouchiy/databricks-ml-examples/tree/master/llm-models/embedding/e5/multilingual-e5-large

# COMMAND ----------

# DBTITLE 1,基盤モデル "databricks-bge-large-en" を使用
import mlflow.deployments
deploy_client = mlflow.deployments.get_deploy_client("databricks")

response = deploy_client.predict(
  endpoint = embedding_endpoint_name, 
  inputs = {
    "input": [
      "Apache Sparkとはなんですか?", 
      "ビッグデータとはなんですか？"
    ]
  }
)
embeddings = [e for e in response['data'][0]['embedding']]

print(embeddings)

# COMMAND ----------

# MAGIC %md
# MAGIC ### 2.1. ベクターサーチのエンドポイントを作成
# MAGIC
# MAGIC ベクターサーチのエンドポイントは[GUIからも作成](https://docs.databricks.com/ja/generative-ai/create-query-vector-search.html#create-a-vector-search-endpoint-using-the-ui)できますが、本デモでは再現性の確保のためにAPIベースの方法をご紹介いたします。

# COMMAND ----------

# DBTITLE 0,ベクターサーチ・エンドポイントの作成
from databricks.vector_search.client import VectorSearchClient
vsc = VectorSearchClient()

if VECTOR_SEARCH_ENDPOINT_NAME not in [e['name'] for e in vsc.list_endpoints().get('endpoints', [])]:
    vsc.create_endpoint(name=VECTOR_SEARCH_ENDPOINT_NAME, endpoint_type="STANDARD")

wait_for_vs_endpoint_to_be_ready(vsc, VECTOR_SEARCH_ENDPOINT_NAME)
print(f"Endpoint named {VECTOR_SEARCH_ENDPOINT_NAME} is ready.")

# COMMAND ----------

# MAGIC %md 
# MAGIC
# MAGIC エンドポイントは [Vector Search Endpoints UI](#/setting/clusters/vector-search) で確認できます。エンドポイント名をクリックすると、そのエンドポイントによって提供されているすべてのインデックスが表示されます。

# COMMAND ----------

# MAGIC %md
# MAGIC ### 2.2. ベクターサーチのインデックスを作成

# COMMAND ----------

from databricks.sdk import WorkspaceClient
import databricks.sdk.service.catalog as c

#インデックスを作成したいテーブル
source_table_fullname = f"{catalog}.{schema}.{embed_table_name}"

# インデックスを格納する場所
vs_index_fullname = f"{catalog}.{schema}.{embed_table_name}_vs_index"

# 同名のインデックスの存在有無をチェックし、存在すれば一度削除
if index_exists(vsc, VECTOR_SEARCH_ENDPOINT_NAME, vs_index_fullname):
  print(f"Deleting index {vs_index_fullname} on endpoint {VECTOR_SEARCH_ENDPOINT_NAME}...")
  vsc.delete_index(VECTOR_SEARCH_ENDPOINT_NAME, vs_index_fullname)

# インデックスが存在しなければ作成
print(f"Creating index {vs_index_fullname} on endpoint {VECTOR_SEARCH_ENDPOINT_NAME}...")
vsc.create_delta_sync_index(
  endpoint_name=VECTOR_SEARCH_ENDPOINT_NAME,
  index_name=vs_index_fullname,
  source_table_name=source_table_fullname,
  pipeline_type="CONTINUOUS",
  primary_key="id",
  embedding_source_column="response",
  embedding_model_endpoint_name=embedding_endpoint_name
)

# else:
  #同期をトリガーして、テーブルに保存された新しいデータでベクターサーチのコンテンツを更新します。
  # vsc.get_index(VECTOR_SEARCH_ENDPOINT_NAME, vs_index_fullname).sync()

#インデックスの準備ができ、すべてEmbeddingが作成され、インデックスが作成されるのをしばし待機する（３分ほど）
wait_for_index_to_be_ready(vsc, VECTOR_SEARCH_ENDPOINT_NAME, vs_index_fullname)
print(f"index {vs_index_fullname} on table {source_table_fullname} is ready")

# COMMAND ----------

# MAGIC %md 
# MAGIC ## 3. 類似コンテンツの検索
# MAGIC
# MAGIC これだけです。Databricksは自動的にDelta Live Tableの新しいエントリーを取り込み、同期します。
# MAGIC
# MAGIC データセットのサイズとモデルのサイズによっては、インデックスの作成に数秒かかることがあります。
# MAGIC
# MAGIC 試しに類似コンテンツを検索してみましょう。
# MAGIC
# MAGIC *Note:`similarity_search` は filters パラメータもサポートしています。これは、RAGシステムにセキュリティレイヤーを追加するのに便利です。誰がエンドポイントへのアクセスを行うかに基づいて、機密性の高いコンテンツをフィルタリングすることができます（例えば、ユーザー情報に基づいて特定の部署をフィルタリングするなど）。*

# COMMAND ----------

import mlflow.deployments
deploy_client = mlflow.deployments.get_deploy_client("databricks")

question = "コスパの良いエアコンはどれですか？"

vs_index = vsc.get_index(VECTOR_SEARCH_ENDPOINT_NAME, vs_index_fullname)
results = vs_index.similarity_search(
  query_text=question,
  columns=["query", "response"],
  num_results=3)
docs = results.get('result', {}).get('data_array', [])
docs

# COMMAND ----------

# MAGIC %md 
# MAGIC ## お疲れ様でした！
