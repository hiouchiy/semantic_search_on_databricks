# Databricks notebook source
# MAGIC %md 
# MAGIC ### 環境
# MAGIC - Runtime: 14.2 ML
# MAGIC - Node type: i3.2xlarge (Single Node)

# COMMAND ----------

# MAGIC %md
# MAGIC ## 0. 事前準備

# COMMAND ----------

# DBTITLE 1,必要な外部ライブラリのインストール 
# MAGIC %pip install mlflow==2.9.0 lxml==4.9.3 transformers==4.30.2 langchain==0.0.344 databricks-vectorsearch==0.22 databricks-sdk==0.12.0 databricks-sql-connector
# MAGIC dbutils.library.restartPython()

# COMMAND ----------

# DBTITLE 1,コンフィグ(環境に合わせて修正してください）
catalog = "hiroshi"
dbName = db = "airbricks"
raw_data_table_name = "raw_query_data_table"
embed_table_name = "airbricks_documentation"

VECTOR_SEARCH_ENDPOINT_NAME="dbdemos_vs_endpoint"

embedding_endpoint_name = "databricks-bge-large-en"

# COMMAND ----------

# DBTITLE 1,カタログ初期化及びデモ用のインポートとヘルパーのインストール
# MAGIC %run ./_resources/00-init $reset_all_data=false

# COMMAND ----------

# MAGIC %md
# MAGIC ## 1. データのダウンロードとテーブル化

# COMMAND ----------

# MAGIC %md
# MAGIC ### 1.1. 元データ格納用のテーブル作成

# COMMAND ----------

spark.conf.set("my.catalogName", catalog)
spark.conf.set("my.schemaName", dbName)
spark.conf.set("my.embbedTableName", embed_table_name)

# COMMAND ----------

# MAGIC %sql
# MAGIC CREATE CATALOG IF NOT EXISTS ${my.catalogName};
# MAGIC USE CATALOG ${my.catalogName};
# MAGIC CREATE SCHEMA IF NOT EXISTS ${my.catalogName}.${my.schemaName};
# MAGIC USE SCHEMA ${my.schemaName};
# MAGIC
# MAGIC DROP TABLE IF EXISTS ${my.embbedTableName};
# MAGIC --Note that we need to enable Change Data Feed on the table to create the index
# MAGIC CREATE TABLE IF NOT EXISTS ${my.embbedTableName} (
# MAGIC   id BIGINT GENERATED BY DEFAULT AS IDENTITY,
# MAGIC   query STRING,
# MAGIC   response STRING
# MAGIC ) TBLPROPERTIES (delta.enableChangeDataFeed = true); 

# COMMAND ----------

# MAGIC %md
# MAGIC ### 1.2. 元データのダウンロードおよびテーブルへのロード

# COMMAND ----------

# Read raw data and create delta table to store it
raw_data_url = "https://raw.githubusercontent.com/hiouchiy/Pratical_RAG_Project/main/airbricks/query.json"
!wget $raw_data_url -O /tmp/query.json

spark.read.option("multiline","true").json('file:/tmp/query.json').write.mode('overwrite').saveAsTable(embed_table_name)

display(spark.table(embed_table_name))

# COMMAND ----------

# MAGIC %md
# MAGIC ## 2. ベクトルDBの作成

# COMMAND ----------

# DBTITLE 1,埋め込みエンドポイントとして基盤モデルdatabricks-bge-large-enを使用
import mlflow.deployments
deploy_client = mlflow.deployments.get_deploy_client("databricks")

# カスタムEmbeddingモデル
response = deploy_client.predict(
  endpoint = embedding_endpoint_name, 
  inputs = {"input": ["Apache Sparkとはなんですか?", "ビッグデータとはなんですか？"]}
)
embeddings = [e for e in response['data'][0]['embedding']]

print(embeddings)

# COMMAND ----------

# DBTITLE 1,Vector searchエンドポイントの作成
from databricks.vector_search.client import VectorSearchClient
vsc = VectorSearchClient()

if VECTOR_SEARCH_ENDPOINT_NAME not in [e['name'] for e in vsc.list_endpoints().get('endpoints', [])]:
    vsc.create_endpoint(name=VECTOR_SEARCH_ENDPOINT_NAME, endpoint_type="STANDARD")

wait_for_vs_endpoint_to_be_ready(vsc, VECTOR_SEARCH_ENDPOINT_NAME)
print(f"Endpoint named {VECTOR_SEARCH_ENDPOINT_NAME} is ready.")

# COMMAND ----------

# MAGIC %md 
# MAGIC
# MAGIC エンドポイントは [Vector Search Endpoints UI](#/setting/clusters/vector-search) で確認できます。エンドポイント名をクリックすると、そのエンドポイントによって提供されているすべてのインデックスが表示されます。

# COMMAND ----------

# DBTITLE 1,エンドポイントを使って自己管理型ベクターサーチインデックスを作成
from databricks.sdk import WorkspaceClient
import databricks.sdk.service.catalog as c

#The table we'd like to index
source_table_fullname = f"{catalog}.{db}.{embed_table_name}"
# Where we want to store our index
vs_index_fullname = f"{catalog}.{db}.{embed_table_name}_vs_index"

if index_exists(vsc, VECTOR_SEARCH_ENDPOINT_NAME, vs_index_fullname):
  print(f"Deleting index {vs_index_fullname} on endpoint {VECTOR_SEARCH_ENDPOINT_NAME}...")
  vsc.delete_index(VECTOR_SEARCH_ENDPOINT_NAME, vs_index_fullname)

# if not index_exists(vsc, VECTOR_SEARCH_ENDPOINT_NAME, vs_index_fullname):
print(f"Creating index {vs_index_fullname} on endpoint {VECTOR_SEARCH_ENDPOINT_NAME}...")
vsc.create_delta_sync_index(
  endpoint_name=VECTOR_SEARCH_ENDPOINT_NAME,
  index_name=vs_index_fullname,
  source_table_name=source_table_fullname,
  pipeline_type="CONTINUOUS",
  primary_key="id",
  embedding_source_column="response",
  embedding_model_endpoint_name=embedding_endpoint_name
)

# else:
  #同期をトリガーして、テーブルに保存された新しいデータでベクターサーチのコンテンツを更新します。
  # vsc.get_index(VECTOR_SEARCH_ENDPOINT_NAME, vs_index_fullname).sync()

#インデックスの準備ができ、すべてエンベッディングが作成され、インデックスが作成されるのを待ちましょう。
wait_for_index_to_be_ready(vsc, VECTOR_SEARCH_ENDPOINT_NAME, vs_index_fullname)
print(f"index {vs_index_fullname} on table {source_table_fullname} is ready")

# COMMAND ----------

# MAGIC %md 
# MAGIC ## 3. 類似コンテンツの検索
# MAGIC
# MAGIC これだけです。Databricksは自動的にDelta Live Tableの新しいエントリーを取り込み、同期します。
# MAGIC
# MAGIC データセットのサイズとモデルのサイズによっては、インデックスの作成に数秒かかることがあります。
# MAGIC
# MAGIC 試しに類似コンテンツを検索してみましょう。
# MAGIC
# MAGIC *Note:`similarity_search` は filters パラメータもサポートしています。これは、RAGシステムにセキュリティレイヤーを追加するのに便利です。誰がエンドポイントへのアクセスを行うかに基づいて、機密性の高いコンテンツをフィルタリングすることができます（例えば、ユーザー情報に基づいて特定の部署をフィルタリングするなど）。*

# COMMAND ----------

import mlflow.deployments
deploy_client = mlflow.deployments.get_deploy_client("databricks")

question = "コスパの良いエアコンはどれですか？"

results = vsc.get_index(VECTOR_SEARCH_ENDPOINT_NAME, vs_index_fullname).similarity_search(
  query_text=question,
  columns=["query", "response"],
  num_results=3)
docs = results.get('result', {}).get('data_array', [])
docs

# COMMAND ----------

# MAGIC %md 
# MAGIC ## お疲れ様でした！

# COMMAND ----------


